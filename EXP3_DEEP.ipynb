{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "NAME: VIKASH S\n",
        "\n",
        "REGISTER NUMBER: 212222240115\n"
      ],
      "metadata": {
        "id": "hlow8UgKjA7z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSQDkqKTbpSe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.ToTensor()"
      ],
      "metadata": {
        "id": "SjGyLjDSbsYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = datasets.MNIST(root='../Data', train=True, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "NA0TGJjyb-1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = datasets.MNIST(root='../Data', train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "_bOScC33b_ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_data, batch_size=10, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=10, shuffle=False)"
      ],
      "metadata": {
        "id": "zrS_6AcFcj5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 colour channel, 6 filter(output channel) 3 x 3 kernal , stride = 1\n",
        "conv1 = nn.Conv2d(1,6,3,1) # ---> 6 filters ---> pooling ---> conv2\n",
        "# 6 input filters conv1, 16 filters, 3 x 3 kernal, stride = 1\n",
        "conv2 = nn.Conv2d(6,16,3,1)"
      ],
      "metadata": {
        "id": "X_Qma9W8ckv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grab the first MNIST record\n",
        "\n",
        "for i, (X_train, y_train) in enumerate(train_data):\n",
        "    break"
      ],
      "metadata": {
        "id": "EAW1vQXYcoro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvolutionalNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1,6,3,1)\n",
        "        self.conv2 = nn.Conv2d(6,16,3,1)\n",
        "        self.fc1 = nn.Linear(5*5*16,120)\n",
        "        self.fc2 = nn.Linear(120,84)\n",
        "        self.fc3 = nn.Linear(84,10)\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = F.relu(self.conv1(X))\n",
        "        X = F.max_pool2d(X, 2, 2)\n",
        "        X = F.relu(self.conv2(X))\n",
        "        X = F.max_pool2d(X, 2, 2)\n",
        "        X = X.view(-1, 5*5*16)\n",
        "        X = F.relu(self.fc1(X))\n",
        "        X = F.relu(self.fc2(X))\n",
        "        X = self.fc3(X)\n",
        "        return F.log_softmax(X, dim=1)\n"
      ],
      "metadata": {
        "id": "7FE16eDRcr0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "HDk4Y21MdSwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "# Variables ( Trackers)\n",
        "epochs = 5\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_correct = []\n",
        "test_correct = []\n",
        "\n",
        "# for loop epochs\n",
        "for i in range(epochs):\n",
        "\n",
        "    trn_corr = 0\n",
        "    tst_corr = 0\n",
        "\n",
        "\n",
        "    # Run the training batches\n",
        "    for b, (X_train, y_train) in enumerate(train_loader):\n",
        "        b+=1\n",
        "\n",
        "        # Apply the model\n",
        "        y_pred = model(X_train)  # we not flatten X-train here\n",
        "        loss = criterion(y_pred, y_train)\n",
        "\n",
        "\n",
        "        predicted = torch.max(y_pred.data, 1)[1]\n",
        "        batch_corr = (predicted == y_train).sum()  # Trure 1 / False 0 sum()\n",
        "        trn_corr += batch_corr\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print interim results\n",
        "        if b%600 == 0:\n",
        "            print(f'epoch: {i}  batch: {b} loss: {loss.item()}')\n",
        "\n",
        "    train_losses.append(loss)\n",
        "    train_correct.append(trn_corr)\n",
        "\n",
        "    # Run the testing batches\n",
        "    with torch.no_grad():\n",
        "        for b, (X_test, y_test) in enumerate(test_loader):\n",
        "\n",
        "            # Apply the model\n",
        "            y_val = model(X_test)\n",
        "\n",
        "            # Tally the number of correct predictions\n",
        "            predicted = torch.max(y_val.data, 1)[1]\n",
        "            tst_corr += (predicted == y_test).sum()\n",
        "\n",
        "    loss = criterion(y_val, y_test)\n",
        "    test_losses.append(loss)\n",
        "    test_correct.append(tst_corr)\n",
        "\n",
        "current_time = time.time()\n",
        "total = current_time - start_time\n",
        "print(f'Training took {total/60} minutes')"
      ],
      "metadata": {
        "id": "z4r1QhNucyMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detach and convert to NumPy\n",
        "train_losses = [t.detach().numpy() for t in train_losses]\n",
        "test_losses = [t.detach().numpy() for t in test_losses]\n",
        "\n",
        "plt.plot(train_losses, label='training loss')\n",
        "plt.plot(test_losses, label='validation loss')\n",
        "plt.title('Loss at the end of each epoch')\n",
        "plt.legend();\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dgtOjNsseRlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot([t/600 for t in train_correct], label='training accuracy')\n",
        "plt.plot([t/100 for t in test_correct], label='validation accuracy')\n",
        "plt.title('Accuracy at the end of each epoch')\n",
        "plt.legend();\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e1bckGkbeaxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the data all at once, not in batches\n",
        "test_load_all = DataLoader(test_data, batch_size=10000, shuffle=False)"
      ],
      "metadata": {
        "id": "zi7Ex8VIegFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    for X_test, y_test in test_load_all:\n",
        "        y_val = model(X_test)  # we don't flatten the data this time\n",
        "        predicted = torch.max(y_val,1)[1]\n",
        "        correct += (predicted == y_test).sum()\n"
      ],
      "metadata": {
        "id": "GTAJyrdGeiJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print a row of values for reference\n",
        "np.set_printoptions(formatter=dict(int=lambda x: f'{x:4}'))\n",
        "print(np.arange(10).reshape(1,10))\n",
        "print()\n",
        "\n",
        "# print the confusion matraix\n",
        "print(confusion_matrix(predicted.view(-1), y_test.view(-1)))"
      ],
      "metadata": {
        "id": "ehW93LAxejlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# single image for test\n",
        "plt.imshow(test_data[2019][0].reshape(28,28))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8HsJ0WPzishl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    new_prediction = model(test_data[2019][0].view(1,1,28,28))"
      ],
      "metadata": {
        "id": "qYimTjaAjdtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_prediction.argmax()"
      ],
      "metadata": {
        "id": "8TkK3CRAje-y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}